# 代码说明

## 环境配置
<!-- 注明python、pytorch等依赖的版本，并对init.sh每一步进行描述，或者在init.sh中对每一步添加注释 -->
conda环境以及依赖的包和对应版本（详见requirements.txt），运行init.sh创建conda环境，激活环境，然后自动安装列表中的依赖包；   
部分依赖包需要pip安装和对应版本（详见requirements_pip.txt），运行init.sh时，会在安装完conda环境依赖后，自动安装pip的依赖包；    
ps:如果发生错误提示，建议手动安装各依赖包咯    

## 数据
未使用除比赛训练数据和测试数据以外的任何数据和预训练模型

## 预训练模型
未使用除比赛训练数据和测试数据以外的任何数据和预训练模型

## 算法
### 整体思路介绍
不同于大部分选手基于更大的预训练语言模型再预训练或者微调的方式，本队伍方法基于baseline的encoder-decoder模型，进一步完善了流程，整体思路为预训练-任务微调的策略；  
1.预训练部分：*************************************************************  
    预训练数据来自比赛的训练集和测试集数据；    
    预训练的策略有：随机动态长度的MASK；随机乱序；随机重复赋值；随机倒序；  
    对预训练的encoder和decoder部分都进行了充分的预训练；   
2.任务微调部分：*************************************************************    
    任务微调训练流程采用了部分训练策略：    
    ①标签软化 labelsmooth，增强模型拟合泛化能力，软化的超参采用随机采样的方式   
    ②对抗训练 FGM，增强模型embedding层的编码泛化能力，对抗超参采用随机采样的方式    
    ③参数平滑 EMA，协助模型迭代找到更优的极值   
    ④训练序列随机处理，处理策略有：随机删除；随机动态长度MASK；随机重复赋值；随机乱序；    
3.模型部分：*************************************************************    
    简单修改了baseline的模型超参：  
    ①encoder和decoder的transformer层数提升为12；    
    ②embedding的特征维度减少为128；     
    ③模型中的激活函数改为Gelu；     
4.预测部分：*************************************************************    
将模型的序列生成策略从贪婪算法改为beamsearch方法    
 
### 网络结构
baseline的模型，encoder-decoder的transformer模块堆叠
### 损失函数
损失函数为交叉熵和基于交叉熵的标签软化
### 数据扩增
数据处理策略详见整体思路
### 模型集成
无模型集成，单模型是本队伍追求，不追求大模型以及模型集成
### 算法的其他细节
无
## 训练流程
执行后，按顺序依次分别执行：    
①生成预训练数据     
②划分训练和验证数据集   
③分析数据的数值和序列长度   
④预训练     
⑤任务微调   
## 测试流程
执行后，给定测试文本，利用训练好的模型进行预测并保存结果
## 其他注意事项
验证数据划分按照9：1的比例
